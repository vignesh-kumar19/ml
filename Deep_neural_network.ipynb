{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*0mia7BQKjUAuXeqZ.jpeg\", alt=\"Deep neural n/w\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X (numpy array)  : input - row as features / dimension, column as examples\n",
    "Y (numpy array)  : output - column as examples\n",
    "m (int)          : number of examples\n",
    "features (int)   : number of features\n",
    "L (int)          : number of Deep neural Layers\n",
    "n (list)         : number of units/nodes in each layer n[0] is input features. Here input(X) has 3 features\n",
    "dense_act (str)  : activation function of dense layer except output layer\n",
    "op_act (str)     : activation function of output layer\n",
    "alpha (int)      : learning rate\n",
    "init_w_rate (int): rate of initialise weight\n",
    "iterations (int) : number of iterations for gradient descent\n",
    "'''\n",
    "X = np.array([(1.0, 1.0, 0.0, 0.0), (1.0, 0.0, 0.0, 0.0), (1.0, 0.0, 0.0, 0.0)])\n",
    "Y = np.array([[(1.0), (1.0), (0.0), (0.0)]])\n",
    "dense_act = 'tanh'\n",
    "op_act = 'sigmoid'\n",
    "# Hyperparameters\n",
    "m = 4\n",
    "features = 3\n",
    "L = 4\n",
    "alpha = 0.01\n",
    "init_w_rate = 1\n",
    "iterations = 1000\n",
    "# n[0] input features\n",
    "# n[1] number of units in 1st hidden layer\n",
    "# n[2] number of units in 2nd hidden layer and so on\n",
    "# n[L-1] number of units in o/p layer\n",
    "n = [3, 5, 5, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {}\n",
    "A[0] = X\n",
    "Z = {}\n",
    "dZ = {}\n",
    "dW = {}\n",
    "db = {}\n",
    "dA = {}\n",
    "# Parameters\n",
    "W = []\n",
    "b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialise weights and biases for all layers'''\n",
    "\n",
    "def init_w_b():\n",
    "    for i in range(len(n)):\n",
    "        W.append(np.random.randn(n[i], n[i-1]) * init_w_rate)\n",
    "        b.append(np.zeros((n[i], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Activation function '''\n",
    "\n",
    "def activation_function(name, z):\n",
    "    if name == 'tanh':\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    elif name == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Derivative of activation function '''\n",
    "\n",
    "def derivative_activation_function(name, z):\n",
    "    if name == 'tanh':\n",
    "        return 1 - (np.tanh(z) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Forward propagation '''\n",
    "\n",
    "def forward_propagation():\n",
    "    for i in range(1, L+1):\n",
    "        lin_fun = np.dot(W[i], A[i-1]) + b[i]\n",
    "        Z[i] = (lin_fun)\n",
    "        if i != L:\n",
    "            A[i] = (activation_function(dense_act, lin_fun))\n",
    "        else:\n",
    "            A[i] = (activation_function(op_act,lin_fun))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Back propagation '''\n",
    "\n",
    "def backward_propagation():\n",
    "    dZ[L] = A[L] - Y\n",
    "    for i in range(L, 0, -1):\n",
    "        dW[i] = (np.dot(dZ[i], A[i-1].T)) / m\n",
    "        db[i] = (np.sum(dZ[i], axis = 1, keepdims=True)) / m\n",
    "        if i != 1:\n",
    "            dA[i-1] = np.dot(W[i].T, dZ[i])\n",
    "            dZ[i-1] = dA[i-1] * derivative_activation_function('tanh', Z[i-1])\n",
    "    \n",
    "    for k in dW.keys():\n",
    "        W[k] = W[k] - (alpha * dW[k])\n",
    "        b[k] = b[k] - (alpha * db[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing input, weights, biases to the Deep neural network and predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/p: \t\t O/p: \tPredicted O/P:\n",
      "1.0, 1.0, 1.0  - 1.0 -  0.6826161148932921\n",
      "1.0, 0.0, 0.0  - 1.0 -  0.6439293428964054\n",
      "0.0, 0.0, 0.0  - 0.0 -  0.5\n",
      "0.0, 0.0, 0.0  - 0.0 -  0.5\n"
     ]
    }
   ],
   "source": [
    "forward_propagation()\n",
    "print(\"I/p: \\t\\t O/p: \\tPredicted O/P:\")\n",
    "for i in range(m):\n",
    "    print(\"{}, {}, {}  - {} -  {}\".format(X[0][i], X[1][i], X[2][i], Y[0][i], A[L][0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning weights and biases for accurate prediction through gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.13801770441864267\n",
      "Loss:  0.07908819718325892\n",
      "Loss:  0.053566185754555004\n",
      "Loss:  0.039774061208551995\n",
      "Loss:  0.0313323317161612\n",
      "Loss:  0.025706220225542833\n",
      "Loss:  0.021720166302454307\n",
      "Loss:  0.018763434954085983\n",
      "Loss:  0.016490761377208635\n",
      "Loss:  0.014693709304408139\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    forward_propagation()\n",
    "    if i % 100 == 0:\n",
    "        J = (-((Y * np.log(A[L])) + ((1 - Y) * np.log(1 - A[L])))) / m\n",
    "        J = J.sum() / m\n",
    "        print(\"Loss: \", J )\n",
    "    backward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After few iterations, now predicted values for all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I/p: \t\t O/p: \tPredicted O/P:\n",
      "1.0, 1.0, 1.0  - 1.0 -  0.9491727061764781\n",
      "1.0, 0.0, 0.0  - 1.0 -  0.9458113179463827\n",
      "0.0, 0.0, 0.0  - 0.0 -  0.050651112743690364\n",
      "0.0, 0.0, 0.0  - 0.0 -  0.050651112743690364\n"
     ]
    }
   ],
   "source": [
    "forward_propagation()\n",
    "print(\"I/p: \\t\\t O/p: \\tPredicted O/P:\")\n",
    "for i in range(m):\n",
    "    print(\"{}, {}, {}  - {} -  {}\".format(X[0][i], X[1][i], X[2][i], Y[0][i], A[L][0][i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
